"""
Phi-3 LLMã‚µãƒ¼ãƒ“ã‚¹ - Ollamaçµ±åˆ
è¦ä»¶å®šç¾©æ›¸æº–æ‹ : ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ç”Ÿæˆ
"""

import asyncio
import json
from typing import Optional, List, Dict
import logging
from datetime import datetime

from fastapi import FastAPI, HTTPException
import httpx
from loguru import logger

# FastAPI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
app = FastAPI(
    title="Phi-3 LLM Service",
    description="Ollama ã‚’ä½¿ç”¨ã—ãŸå¯¾è©±ç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹ï¼ˆPhi-3 mini-4k-instructï¼‰",
    version="1.0.0"
)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
conversation_history: List[Dict] = []

# è¨­å®š
OLLAMA_HOST = "127.0.0.1:11434"
MODEL_NAME = "phi3:mini"

@app.on_event("startup")
async def startup_event():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚ã®åˆæœŸåŒ–"""
    logger.info("ğŸ¤– Phi-3 LLM Service èµ·å‹•ä¸­...")
    
    # Phi-3ãƒ¢ãƒ‡ãƒ«ã®ç¢ºèªãƒ»è‡ªå‹•ãƒ—ãƒ«
    try:
        async with httpx.AsyncClient() as client:
            # ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’ç¢ºèª
            response = await client.get(f"http://{OLLAMA_HOST}/api/tags", timeout=10.0)
            if response.status_code == 200:
                models = response.json().get("models", [])
                model_names = [model["name"] for model in models]
                
                if MODEL_NAME not in model_names:
                    logger.info(f"ğŸ“¥ Phi-3ãƒ¢ãƒ‡ãƒ« '{MODEL_NAME}' ã‚’ãƒ—ãƒ«ä¸­...")
                    # ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒ«ï¼ˆéåŒæœŸã§é–‹å§‹ï¼‰
                    await client.post(
                        f"http://{OLLAMA_HOST}/api/pull",
                        json={"name": MODEL_NAME},
                        timeout=300.0  # 5åˆ†ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    )
                    logger.info("âœ… Phi-3ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒ«å®Œäº†")
                else:
                    logger.info(f"âœ… Phi-3ãƒ¢ãƒ‡ãƒ« '{MODEL_NAME}' åˆ©ç”¨å¯èƒ½")
            
    except Exception as e:
        logger.warning(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«ç¢ºèª/ãƒ—ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
    
    logger.info("ğŸ‰ Phi-3 LLM Service åˆæœŸåŒ–å®Œäº†")

@app.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    # Ollamaæ¥ç¶šç¢ºèª
    ollama_connected = False
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(f"http://{OLLAMA_HOST}/api/tags", timeout=5.0)
            ollama_connected = response.status_code == 200
    except:
        pass
    
    return {
        "status": "ok",
        "service": "phi3-llm",
        "model": MODEL_NAME,
        "ollama_connected": ollama_connected,
        "conversation_length": len(conversation_history),
        "timestamp": datetime.now().isoformat()
    }

@app.post("/generate")
async def generate_text(request: dict):
    """ãƒ†ã‚­ã‚¹ãƒˆç”ŸæˆAPI"""
    try:
        user_text = request.get("text", "")
        if not user_text:
            raise HTTPException(status_code=400, detail="ãƒ†ã‚­ã‚¹ãƒˆãŒå¿…è¦ã§ã™")
        
        # ä¼šè©±å±¥æ­´ã‹ã‚‰æ–‡è„ˆã‚’æ§‹ç¯‰
        context = ""
        for entry in conversation_history[-3:]:  # ç›´è¿‘3å›
            context += f"User: {entry['user']}\\nAI: {entry['ai']}\\n"
        
        # Phi-3ç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        if context.strip():
            prompt = f"""You are a helpful English conversation partner for language learning. Keep responses natural and engaging.

Recent conversation:
{context}

User: {user_text}
Assistant:"""
        else:
            prompt = f"""You are a helpful English conversation partner for language learning. Keep responses natural and engaging.

User: {user_text}
Assistant:"""
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"http://{OLLAMA_HOST}/api/generate",
                json={
                    "model": MODEL_NAME,
                    "prompt": prompt,
                    "stream": False,  # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸€æ—¦ç„¡åŠ¹åŒ–
                    "options": {
                        "temperature": 0.3,  # ã‚ˆã‚Šæ±ºå®šçš„ãªå¿œç­”ã§é«˜é€ŸåŒ–
                        "num_predict": 30,   # ãƒˆãƒ¼ã‚¯ãƒ³æ•°å‰Šæ¸›ã§é«˜é€ŸåŒ–
                        "top_p": 0.9,       # ã‚ˆã‚Šé›†ä¸­ã—ãŸå¿œç­”
                        "top_k": 20,        # èªå½™é¸æŠã‚’åˆ¶é™ã—ã¦é«˜é€ŸåŒ–
                        "num_ctx": 1024     # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’åˆ¶é™
                    }
                },
                timeout=15.0  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’çŸ­ç¸®
            )
            
            if response.status_code == 200:
                result = response.json()
                ai_response = result.get("response", "").strip()
                
                # ä¼šè©±å±¥æ­´ã«è¿½åŠ 
                conversation_history.append({
                    "user": user_text,
                    "ai": ai_response,
                    "timestamp": datetime.now().isoformat()
                })
                
                # æœ€å¾Œã®5å›ã®ã¿ä¿æŒ
                if len(conversation_history) > 5:
                    conversation_history[:] = conversation_history[-5:]
                
                return {
                    "input": user_text,
                    "response": ai_response if ai_response else "I'm sorry, I didn't understand that. Could you try again?",
                    "conversation_history": conversation_history
                }
            else:
                return {
                    "input": user_text,
                    "response": "I'm having trouble connecting to my language model. Please try again.",
                    "conversation_history": conversation_history
                }
                
    except Exception as e:
        logger.error(f"âŒ LLMå¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/reset")
async def reset_conversation():
    """ä¼šè©±å±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆ"""
    global conversation_history
    conversation_history = []
    return {"message": "ä¼šè©±å±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8002)